{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 updates 1257 15.0 % update_rate: 0.2514\n",
      "epoch 2 updates 1221 15.1 % update_rate: 0.2478\n",
      "epoch 3 updates 1177 14.8 % update_rate: 0.24366666666666667\n",
      "epoch 4 updates 1170 14.7 % update_rate: 0.24125\n",
      "epoch 5 updates 1172 14.8 % update_rate: 0.23988\n",
      "epoch 6 updates 1185 15.2 % update_rate: 0.2394\n",
      "epoch 7 updates 1165 15.5 % update_rate: 0.23848571428571427\n",
      "epoch 8 updates 1185 15.9 % update_rate: 0.2383\n",
      "epoch 9 updates 1184 15.8 % update_rate: 0.23813333333333334\n",
      "epoch 10 updates 1181 15.7 % update_rate: 0.23794\n",
      "epoch 11 updates 1156 15.6 % update_rate: 0.23732727272727272\n",
      "epoch 12 updates 1138 15.7 % update_rate: 0.23651666666666665\n",
      "epoch 13 updates 1165 15.7 % update_rate: 0.23624615384615386\n",
      "epoch 14 updates 1174 15.7 % update_rate: 0.23614285714285715\n",
      "epoch 15 updates 1166 15.6 % update_rate: 0.23594666666666667\n",
      "epoch 16 updates 1162 15.6 % update_rate: 0.235725\n",
      "epoch 17 updates 1183 15.4 % update_rate: 0.23577647058823528\n",
      "epoch 18 updates 1159 15.4 % update_rate: 0.23555555555555555\n",
      "epoch 19 updates 1156 15.4 % update_rate: 0.2353263157894737\n",
      "epoch 20 updates 1146 15.5 % update_rate: 0.23502\n",
      "epoch 21 updates 1151 15.5 % update_rate: 0.2347904761904762\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8c55de6044f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-8c55de6044f7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperceptron_avg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;31m# Calculate error rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         print(\"epoch\", epoch, \n",
      "\u001b[0;32m<ipython-input-3-8c55de6044f7>\u001b[0m in \u001b[0;36mperceptron_avg\u001b[0;34m(X, Y, epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For each observation and its index in the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Store label for the current observation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mpredicted_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Calculate the prediction for y.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_y\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m  \u001b[0;31m# Create a flag to check if the prediction is right.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Basic Perceptron\n",
    "# Reference: https://stackoverflow.com/questions/47213847/how-to-implement-perceptron-in-python\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import os\n",
    "# os.chdir(\"/home/minion/Desktop/ML/HW2\")\n",
    "np.random.seed(123)\n",
    "\n",
    "# Need to print the percent positive\n",
    "\n",
    "\n",
    "def perceptron_avg(X, Y, epochs):\n",
    "    ones = np.ones(X.shape[0]).reshape(X.shape[0], 1)  # Creates an array of ones for the bias row.\n",
    "    X1 = np.append(ones, X, axis=1)  # Append the data matrix to the new row of ones.\n",
    "\n",
    "    w = np.zeros(X1.shape[1])  # Create an array of zeros to store the weights.\n",
    "    w_a = np.zeros(X1.shape[1])  # average weights\n",
    "    final_iter = epochs  # Assign final iteration variable to the specified epoch.\n",
    "    \n",
    "    c=0\n",
    "    update_counter=0\n",
    "    for epoch in range(epochs):  # For each epoch until reaching the maximum specified epoch.\n",
    "        misclassified = 0\n",
    "\n",
    "        for i, x in enumerate(X1):  # For each observation and its index in the training set\n",
    "            y = Y[i]  # Store label for the current observation.\n",
    "            predicted_y = np.dot(x, w)  # Calculate the prediction for y.\n",
    "            h = predicted_y*y  # Create a flag to check if the prediction is right.\n",
    "\n",
    "            if h <= 0:  # If prediction is below or equal to zero, then it is wrong.\n",
    "                w = w + x*y  # Update the weight to shift and rotate the plane to be more accurate.\n",
    "                w_a = w_a + c*x*y\n",
    "                misclassified += 1  # Add 1 to the misclassification counter. \n",
    "            c+=1   \n",
    "            \n",
    "        update_counter+=misclassified\n",
    "            # else: if the prediction is above 0, then it is correct, and we can proceed.\n",
    "        # The process is repeated until all observations have been iterated over, for the requested number of epochs.\n",
    "        if misclassified == 0:  # If we converge, we don't need to continue.\n",
    "            final_iter = epoch\n",
    "            break\n",
    "\n",
    "    updates = misclassified\n",
    "    return c*w-w_a, final_iter, updates,update_counter/(final_iter*X1.shape[0])  # Return an array of weight and the number of epochs went.\n",
    "\n",
    "\n",
    "def calculate_error_rate(X, Y, w):\n",
    "    ones = np.ones(X.shape[0]).reshape(X.shape[0], 1)  # Creates an array of ones for the bias row.\n",
    "    X1 = np.append(ones, X, axis=1)  # Append the data matrix to the new row of ones.\n",
    "\n",
    "    misclassified = 0\n",
    "\n",
    "    for i, x in enumerate(X1):  # For each observation and its index in the dev set\n",
    "        y = Y[i]  # Store label for the current observation.\n",
    "        predicted_y = np.dot(x, w)  # Calculate the prediction for y.\n",
    "        h = predicted_y * y  # Create a flag to check if the prediction is right.\n",
    "\n",
    "        if h <= 0:  # If prediction is below or equal to zero, then it is wrong.\n",
    "            misclassified += 1  # Add 1 to the misclassification counter.\n",
    "\n",
    "    return misclassified/(X.shape[0])\n",
    "\n",
    "\n",
    "def load_binarized_features(filename, num_rows, num_features):\n",
    "\n",
    "    lines = open(filename).readlines()\n",
    "    lines = [line.strip().split(\", \") for line in lines]\n",
    "\n",
    "    loaded_data = [[value for idx, value in enumerate(line) if idx not in [9]] for line in lines] #9 is target\n",
    "\n",
    "    mapping = {}\n",
    "    new_data = []\n",
    "\n",
    "    for row in loaded_data:\n",
    "        new_row = []\n",
    "        for j, x in enumerate(row):\n",
    "            feature = (j, x)\n",
    "            if feature not in mapping:\n",
    "                mapping[feature] = len(mapping)  # insert a new feature into the index\n",
    "            new_row.append(mapping[feature])\n",
    "        new_data.append(new_row)\n",
    "\n",
    "    binary_data = np.zeros((num_rows, num_features))\n",
    "    # store normalized numerical values\n",
    "    for idx, row in enumerate(new_data):\n",
    "        for jdx in row:\n",
    "            binary_data[idx][jdx] = 1\n",
    "\n",
    "    return binary_data\n",
    "\n",
    "\n",
    "def load_labels(filename):\n",
    "    lines = open(filename).readlines()\n",
    "    lines = [line.strip().split(\", \") for line in lines]\n",
    "\n",
    "    labels = [[value for idx, value in enumerate(line) if idx in [9]] for line in lines]\n",
    "    labels = [val for sublist in labels for val in sublist]\n",
    "    y_array = []\n",
    "\n",
    "    for label in labels:\n",
    "        if label == '<=50K':\n",
    "            y_array.append(-1)\n",
    "        elif label == '>50K':\n",
    "            y_array.append(1)\n",
    "\n",
    "    y_array = np.array(y_array)\n",
    "    return y_array\n",
    "\n",
    "def main():\n",
    "\n",
    "    filenames = ['income.train.txt.5k', 'income.dev.txt']\n",
    "    with open('combined.txt', 'w') as outfile:\n",
    "        for f in filenames:\n",
    "            with open(f) as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "\n",
    "    filename = 'combined.txt'\n",
    "    num_rows = 6000\n",
    "    num_features = 233 ### WHY?\n",
    "\n",
    "    binarized_features = load_binarized_features(filename, num_rows, num_features)\n",
    "\n",
    "    xTrain = binarized_features[0:5000]\n",
    "    xDev = binarized_features[5000:6001]\n",
    "\n",
    "    labels = load_labels(filename)\n",
    "\n",
    "    yTrain = np.array(labels[0:5000])\n",
    "    yDev = np.array(labels[5000:6001])\n",
    "\n",
    "\n",
    "    # Fit perceptron\n",
    "    max_epochs = 100\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        w, final_iter, updates, update_rate = perceptron_avg(xTrain, yTrain, epoch)\n",
    "        # Calculate error rate\n",
    "        print(\"epoch\", epoch, \n",
    "              \"updates\", updates, round(calculate_error_rate(xDev, yDev, w)*100, 2), \"%\",\n",
    "              'update_rate:',update_rate)\n",
    "        epoch += 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# Averaged Perceptron\n",
    "# References:\n",
    "# https://stackoverflow.com/questions/47213469/how-to-implement-averaged-perceptron-in-python-without-scikit-learn\n",
    "# https://stackoverflow.com/questions/47213847/how-to-implement-perceptron-in-python\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir(\"/home/minion/Desktop/ML/HW2\")\n",
    "\n",
    "\n",
    "def process_data(filename):\n",
    "    X, Y = [], []\n",
    "    for j, line in enumerate(open(filename)):\n",
    "        line = line.strip()\n",
    "        features = line.split(\", \")\n",
    "        feat_vec = np.zeros(dimension)\n",
    "        for i, fv in enumerate(features[:-1]):  # last one is target\n",
    "            if (i, fv) in feature_map:  # ignore unobserved features\n",
    "                feat_vec[feature_map[i, fv]] = 1\n",
    "\n",
    "        X.append(feat_vec)\n",
    "        Y.append(1 if features[-1] == \">50K\" else -1)  # fake for testdata\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "def perceptron_avg(X, Y, epochs):\n",
    "    ones = np.ones(X.shape[0]).reshape(X.shape[0], 1)  # Creates an array of ones for the bias row.\n",
    "    X1 = np.append(ones, X, axis=1)  # Append the data matrix to the new row of ones.\n",
    "\n",
    "    w = np.zeros(X1.shape[1])  # Create an array of zeros to store the weights.\n",
    "    w_a = np.zeros(X1.shape[1])  # average weights\n",
    "    final_iter = epochs  # Assign final iteration variable to the specified epoch.\n",
    "\n",
    "    c = 0\n",
    "    update_counter = 0\n",
    "    for epoch in range(epochs):  # For each epoch until reaching the maximum specified epoch.\n",
    "        misclassified = 0\n",
    "\n",
    "        for i, x in enumerate(X1):  # For each observation and its index in the training set\n",
    "            y = Y[i]  # Store label for the current observation.\n",
    "            predicted_y = np.dot(x, w)  # Calculate the prediction for y.\n",
    "            h = predicted_y * y  # Create a flag to check if the prediction is right.\n",
    "\n",
    "            if h <= 0:  # If prediction is below or equal to zero, then it is wrong.\n",
    "                w = w + x * y  # Update the weight to shift and rotate the plane to be more accurate.\n",
    "                w_a = w_a + c * x * y\n",
    "                misclassified += 1  # Add 1 to the misclassification counter.\n",
    "            c += 1\n",
    "\n",
    "        # else: if the prediction is above 0, then it is correct, and we can proceed.\n",
    "        # The process is repeated until all observations have been iterated over, for the requested number of epochs.\n",
    "        if misclassified == 0:  # If we converge, we don't need to continue.\n",
    "            final_iter = epoch\n",
    "            break\n",
    "\n",
    "    updates = misclassified\n",
    "    return c * w - w_a, final_iter, updates  # Return an array of weight and the number of epochs went.\n",
    "\n",
    "\n",
    "def calculate_error_rate(X, Y, w):\n",
    "    ones = np.ones(X.shape[0]).reshape(X.shape[0], 1)  # Creates an array of ones for the bias row.\n",
    "    X1 = np.append(ones, X, axis=1)  # Append the data matrix to the new row of ones.\n",
    "\n",
    "    misclassified = 0\n",
    "    predictions = []\n",
    "    for i, x in enumerate(X1):  # For each observation and its index in the dev set\n",
    "        y = Y[i]  # Store label for the current observation.\n",
    "        predicted_y = np.dot(x, w)  # Calculate the prediction for y.\n",
    "        predicted_y = 1 if predicted_y > 0 else -1\n",
    "        predictions.append(predicted_y)\n",
    "        h = predicted_y * y  # Create a flag to check if the prediction is right.\n",
    "        if h <= 0:  # If prediction is below or equal to zero, then it is wrong.\n",
    "            misclassified += 1  # Add 1 to the misclassification counter.\n",
    "\n",
    "    positive_percentage = predictions.count(1) / len(predictions)\n",
    "    return misclassified / (X.shape[0]), positive_percentage\n",
    "\n",
    "\n",
    "def get_high_weights_features(w, feature_map):\n",
    "    x = {key: w[value] for key, value in feature_map.items()}\n",
    "    return {k: v for k, v in sorted(x.items(), key=lambda item: item[1])}\n",
    "\n",
    "\n",
    "def predict(X, w):\n",
    "    ones = np.ones(X.shape[0]).reshape(X.shape[0], 1)  # Creates an array of ones for the bias row.\n",
    "    X1 = np.append(ones, X, axis=1)  # Append the data matrix to the new row of ones.\n",
    "\n",
    "    predictions = []\n",
    "    for i, x in enumerate(X1):  # For each observation and its index in the dev set\n",
    "        predicted_y = np.dot(x, w)  # Calculate the prediction for y.\n",
    "        predictions.append(1 if predicted_y > 0 else -1)\n",
    "    positive_percentage = predictions.count(1) / len(predictions)\n",
    "\n",
    "    return predictions, positive_percentage\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    field_value_freqs = defaultdict(lambda: defaultdict(int))  # field_id -> value -> freq\n",
    "    for line in open(\"income.train.txt.5k\"):\n",
    "        line = line.strip()\n",
    "        features = line.split(\", \")[:-1]  # exclude target label\n",
    "        for i, fv in enumerate(features):\n",
    "            field_value_freqs[i][fv] += 1\n",
    "\n",
    "    feature_map = {}\n",
    "    feature_remap = {}\n",
    "    for i, value_freqs in field_value_freqs.items():\n",
    "        for v in value_freqs:\n",
    "            k = len(feature_map)  # bias\n",
    "            feature_map[i, v] = k\n",
    "            feature_remap[k] = i, v\n",
    "\n",
    "    dimension = len(feature_map)  # bias\n",
    "    print(\"dimensionality: %d\" % dimension)  # , feature_map\n",
    "\n",
    "    train_data = process_data(\"income.train.txt.5k\")\n",
    "    dev_data = process_data(\"income.dev.txt\")\n",
    "    test_data = process_data(\"income.test.blind\")\n",
    "\n",
    "    xTrain = train_data[0][:5000]\n",
    "    yTrain = train_data[1][:5000]\n",
    "    xDev = dev_data[0][:1000]\n",
    "    yDev = dev_data[1][:1000]\n",
    "\n",
    "    # Fit perceptron\n",
    "    max_epochs = 5\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        w, final_iter, updates = perceptron_avg(xTrain, yTrain, epoch)\n",
    "        error_rate, positive_percentage = calculate_error_rate(xDev, yDev, w)\n",
    "        print(\"epoch\", epoch, \"--> updates:\", updates, \"(\", round(updates / xTrain.shape[0] * 100, 2), \"% ) dev_err:\",\n",
    "              round(error_rate * 100, 2), \"% (+:\", round(positive_percentage * 100, 2), \"%)\")\n",
    "        epoch += 1\n",
    "\n",
    "    sorted_weights = get_high_weights_features(w, feature_map)\n",
    "    print(sorted_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
